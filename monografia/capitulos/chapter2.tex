\chapter{Theoretical Introduction}

%Quando a introducao estiver pronta adicionar aqui uma breve explciacao sobre a
%estrutura da introducao.

\section{Abstract Syntax}
Robert Harper in~\cite{practicalFoundations} defines Programming Languages
as means of expressing computations in a form comprehensible to
both people and machines and the syntax of a language specifies the means
by which various sorts of phrases (expressions, commands, declarations, and
so forth) may be combined to form programs.

To better understand the syntax, it is usually introduced a few other auxiliary
concepts, they are \textit{concrete syntax}, \textit{abstract syntax} and
\textit{binding}.
The \textit{surface}, or \textit{concrete syntax} is concerned with how
phrases are entered and displayed on a computer. The \textit{surface
syntax} is usually thought of as given by strings of characters from some
alphabet. 
The \textit{structural}, or \textit{abstract syntax} is concerned
with the structure of phrases, specifically how they are composed from
other phrases. 
At this level a phrase is a tree, called \textit{abstract
syntax tree}, whose nodes are operators that combine several phrases to
form another phrase. The \textit{binding} structure of syntax is concerned
with the introduction and use of identifiers how they are declared, and how
declared identifers are to be used. At  this level phrases are abstract
binding trees, which enrich \textit{abstract syntax} trees with the
concepts of binding and scope.

An \textit{abstract syntax tree}, or \textit{ast} for short, is an
ordered tree whose leaves are variables, and whose interior nodes are
operators whose arguments are its children. A \textit{variable} stands
for an unspecified, or generic piece of syntax of specified sort. Ast's
may be combined by an \textit{operator}, which as an \textit{arity}
specifying the sort of the operator and the number and sorts of its
arguments. 

A variable is an unknown object drawn from some domain. The unknown can
become known by substitution of a particular object for all occurrences of
a variable in a formula, thereby specializing a general formaula to a
particular instance. For example, in school, algebra variables range over
real numbers, say, 7 for x to obtain $7^2 + (2 \times 7) + 1$, which may be
simplified according to the laws of arithmetic to obtain 64, which is
indeed ${(7+1)}^2$ as it ought to be, since $x^2 + 2x + 1 = {(x + 1)}^2$ in
general, and hence in any particular case. The core idea is that \textit{a
variable is an unknown, or a placeholder, whose meaning is given by
susbtition}

As an example, consider a language of arithmetic expressions built from numbers
addition and multiplication. The abstract syntax of such a language consists of
a single sort, Exp, generated by these operations:

\begin{itemize}
    \item An operator num[n] of sort Exp for each n $\in$ N\@;
    \item Two operators, plus and times, of sort Exp, each with two
    arguments of sort Exp.
\end{itemize}

The expression $2+(3 * x)$, which involves a variable $x$, would represent by
the ast $plus(num[2]; times(num[3]; x))$, which is written informally as $2 +
(3 \times 4)$.  

\textit{Abstract binding trees}, or \textit{abt's} enrich ast's with
the means to introduce new variables and symbols, called a
\textit{binding}, with a specified range of significance, called its
\textit{scope}. The scope of a binding is an abt within which the bound
identifier may be used, either as a placeholder (in the case of a
variable declaration) or as the index of some operator (in the case of
a symbol declaration). Thus the set of active identifiers may be larger
within a subtree of an abt than it is within the surrounding tree.
Moreover, different subtrees may introduce identifiers with disjoint
scopes. The crucial principle is that any use of an identifier should
be understood as a reference, or abstract pointer, to its binding. One
consequence is that the choice of identifiers is immaterial, so long as
we can always associate a unique binding with each use of an
identifier.

For an example consider the expression $let x be a_1 in a_2$, which
introduces a variable $x$, for use within the expression $a_2$ to stand
for the expression $a_1$. The variable x is bound by the let expression
for use within $a_2$; any use of $x$ within $a_1$ refers to a different
variable that happens to have the same name. For example, in the
expression $let x be 7 in x+x$ ocurrences of x in the addition refer to
the variable introduced by the let. On the other and in the expression
$let be x*x in x+x$, occurrences of x within the multiplication refer
to a different variable than occurring whithin the addition.

\section{Inductive Definitions}
An inductive definition consists of a set of \textit{rules} for deriving
\textit{judgements}, or \textit{assertion}. Judgments are statements
about one or more abt's of some sort.  The rules specify necessary and
sufficient conditions for the validity of a judgement, and hence
determine its meaning.

As examples of \textit{judgements} we have:

\begin{table}[h!]
    \centering
    \begin{tabular}{cc}
        $n$ nat & $n$ is a natural number \\
        $n$ = $n_1$ + $n_2$ & $n$ is the sum of $n_1$ and $n_2$\\
        t type & t is a type\\
        e~: t & expression e has type t\\
        e $ \Downarrow $ v & expression e has type v
    \end{tabular}
    \caption{Judgements Example}
\end{table}

A judgment states that one or more abstract binding trees have a property
or stand in some relation to one another.

\section{Inference Rules}
An \textit{inductive definition} of a judgment form consists of a collection of
\textit{rules} of the form

\begin{mathpar}
\label{natrules1}
    \inferrule{J_1 \and \ldots \and J_k}
    {J}
\end{mathpar}

in which $J$ and $J_1, \cdots J_k$ are all judgments of the form
being defined. The judgments above the horizontal line are called
the \textit{premises} of the rule, and the judgement below the
line is called its \textit{conclusion}. If a rule has no premises
the rule is called an \textit{axiom}; otherwise it is called a
\textit{proper rule}.

For example, the following rules form an inductive definition of the 
judgement form --- nat:

\begin{mathpar}
\label{natrules2}
    \inferrule{~}
    {zero \ nat}
\end{mathpar}

\begin{mathpar} 
    \inferrule{a \ nat}
    {succ~(a) \ nat}
\end{mathpar}

These rules specify that a nat holds whenever either a is zero,
or a is succ~(b) where b nat for some b. Taking these rules to be
exhaustive, it follows that a nat iff a is a natural number.

\section{Derivation}
To show that an iductively defined judgement holds, it is enough to exhibit a
derivation of it. A derivation of a judgement is a finite composition of rules,
starting with axioms and ending with that judgment. It may be thought of as a
tree in which each node is a rule whose children are derivations of its
premises. We sometimes say that a derivation of J is evidence for the validity
of an inductively defined judgement J.

We usually depict derivations as trees with the conclusion at the bottom, and
with the children of a node corresponding to a rule appearing above it as
evidence for the premises of that rule.

For example, this is a derivation of succ~(succ~(succ~(zero))) nat:

\begin{mathpar}
\label{natrules}
    \inferrule*
      {\inferrule* 
        {\inferrule* 
          {\inferrule* {~} {zero \ nat}}
        {succ~(zero)}}
      {succ~(succ~(zero))}}
    {succ~(succ~(succ~(zero)))}
\end{mathpar}~\ref{natrules}

To show that an inductively defined judgment is derivable, we need only find a
derivation for it. There are two main methods for finding derivations, called
\textit{forward chaining}, or \textit{bottom-up construction}, and
\textit{backward chaining}, or \textit{top-down construction}.
Forward chaining starts with the axioms and works forward towards the desired
conclusion, whereas backward chaining starts with the desired conclusion and
works backwards towards the axioms.

More precisely, forward chaining search maintains a set of derivable judgments and
continually extends this set by adding to it the conclusion of any rule all of whose
premises are in that set. Initially, the set is empty; the process terminates when the
desired judgment occurs in the set. Assuming that all rules are considered at every stage,
forward chaining will eventually find a derivation of any derivable judgment, but it is
impossible (in general) to decide algorithmically when to stop extending the set and
conclude that the desired judgment is not derivable. We may go on and on adding more
judgments to the derivable set without ever achieving the intended goal. It is a matter of
understanding the global properties of the rules to determine that a given judgment is
not derivable.


Forward chaining is undirected in the sense that it does not take account of the end
goal when deciding how to proceed at each step. In contrast, backward chaining is
goal-directed. Backward chaining search maintains a queue of current goals,
judgments whose derivations are to be sought. Initially, this set consists solely of the
judgment we wish to derive. At each stage, we remove a judgment from the queue
and consider all rules whose conclusion is that judgment. For each such rule,
we add the premises of that rule to the back of the queue, and continue. If
there is more than one such rule, this process must be repeated, with the same
starting queue, for each candidate rule. The process terminates whenever the
queue is empty, all goals having been achieved; any pending consideration of
candidate rules along the way can be discarded. As with forward chaining,
backward chaining will eventually find a derivation of any derivable judgment, but
there is, in general, no algorithmic method for determining in general whether
the current goal is derivable. If it is not, we may futilely add more and more
judgments to the goal set, never reaching a point at which all goals have been
satisfied.


\section{Rule Induction}

Because an inductive definition specifies the \textit{strongest} judgment form closed under a
collection of rules, we may reason about them by \textit{rule induction}. The principle of rule
induction states that to show that a property $a\ \mathcal{P}$ holds whenever $a
\ \mathsf{J}$ is derivable, it is enough to show that $\mathcal{P}$ is
\textit{closed under}, or \textit{respects}, the rules defining the judgment
form $\mathsf{J}$. More precisely, the property $\mathcal{P}$ respects the rule

\begin{mathpar} 
    \inferrule
    {a_1 \mathsf{J} \ \ldots \ a_k \mathsf{J}}
    {a \mathsf{J}}
\end{mathpar}
if $\mathcal{P}(a)$ holds whenever $\mathcal{P}(a_1),\ldots,\mathcal{P}(a_k)$ do.
The assumptions $\mathcal{P}(a_1),\ldots,\mathcal{P}(a_k)$ are called the
\textit{inductive hypotheses}, and $\mathcal{P}(a)$ is called \textit{inductive
conclusion} of the inference.

The principle of rule induction is simply the expression of the definition of an
inductively defined judgment form as the \textit{strongest} judgment form closed
under the rules comprising the definition. Thus, the judgment form defined by a
set of rules is both (a) closed under those rules, and (b) sufficient for any
other property also closed under those rules. The former means that a derivation
is evidence for the validity of a judgment; the
latter means that we may reason about an inductively defined judgment form by rule induction

When specialized to the nat rules, the principle of rule induction states that
to show $\mathcal{P}(a)$ whenver $a \ nat$, it is enough to show:

\begin{enumerate}
        \item $P(zero)$
        \item for every $a$, if $\mathcal{P}(a)$, then $\mathcal{P}(succ(a))$.
\end{enumerate}
The sufficiency of these conditions is the familiar principle of \textit{mathematical induction}

\section{Hypothetical Judgements}
A \textit{hypothetical judgment} expresses an entailment between one or more
hypotheses and a conclusion. We will consider two notions of entailment, called
\textit{derivability} and \textit{admissibility}. Both express a form of
entailment, but they differ in that derivability is stable under extension with
new rules, admissibility is not. A \textit{general judgment} expresses
the universality, or genericity, of a judgment. There are two forms of general
judgment, the \textit{generic} and the \textit{parametric}. The generic judgment
expresses generality with respect to all substitution instances for variables in
a judgment. The parametric judgment expresses generality with respect to
renamings of symbols.

The hypothetical judgment codifies the rules for expressing the validity of a
conclusion conditional on the validity of one or more hypotheses. There are two
forms of hypothetical judgment that differ according to the sense in which the
conclusion is conditional on the hypotheses. One is stable under extension with
more rules, and the other is not.

\subsection{Derivability}
For a given set $\mathcal{R}$ of rules, we define the \textit{derivability}
judgement, written $J_1,\ldots,J_k \vdash_\mathcal{R} K$, where each $J_i$ and $K$
are basic judgements, to mean that we may derive $K$ from the \textit{expansion}
of the rules $\mathcal{R}$ with the axioms

\begin{mathpar}
            \inferrule{~} {J_1} \quad
            \ldots \quad
            \inferrule{~} {J_k} \quad
\end{mathpar}

We treat the \textit{hypotheses}, or \textit{antecedents}, of the judgment $J_1,
\ldots, J_k$ as “temporary axioms,” and derive the conclusion, or consequent, by
composing rules in $\mathcal{R}$. Thus, evidence for a hypothetical judgment
consists of a derivation of the conclusion from the
hypotheses using the rules in $\mathcal{R}$.
We use capital Greek letters, usually $\Gamma$ or $\Delta$, to stand for a
finite set of basic judgments, and write $\mathcal{R} \bigcup \Gamma$ for the
expansion of $\mathcal{R}$ with an axiom corresponding to each judgment in
$\Gamma$.The judgment $\Gamma \vdash_\mathcal{R} K$ means that $K$ is
derivable from rules $\mathcal{R} \bigcup \Gamma$, and the judgment
$\vdash_\mathcal{R} \Gamma$ means that $\vdash_\mathcal{R} J$ for each in
$\Gamma$. An equivalent way of defining $J_1, \ldots, J_n \vdash_\mathcal{R} J$ is
to say that the rule
\begin{mathpar}
    \inferrule{J_1 \quad \ldots \quad J_n} {J}
\end{mathpar}

is \textit{derivable} from $mathcal{R}$, which means that there is a derivation
of $J$ composed of the rules in $\mathcal{R}$ augmented by treating $J_1, \ldots,
J_n$ as axioms.

For example, consider the derivability judgment.
\begin{displaymath}
    a \ nat \vdash succ(succ(a)) nat
\end{displaymath}
This judgement is valid for any choice of object $a$, as shown by the derivation

\begin{mathpar}
    \inferrule* 
        {\inferrule* 
            {\inferrule* {~} {a \ nat}}
            {succ~(a)}}
        {succ~(succ~(a))}
\end{mathpar}

which composes rules~??, starting with $a \ nat$ as an axiom, and ending with
$succ~(succ~(a)) nat$.

It follows directly from the definition of derivability that it is stable under
extension with new rules.

\begin{theorem}[Stability]
    If $\Gamma \vdash_\mathcal{R} J$, then $\Gamma \vdash_{\mathcal{R} \cup \mathcal{R'}} J$
\end{theorem}

\begin{proof}
    Any derivation of $J$ from $\mathcal{R} \bigcup \Gamma$ is also a derivation
    from $(\mathcal{R} \cup \mathcal{R'})\cup \Gamma$, because any rule in
    $\mathcal{R}$ is also a rule in $\mathcal{R} \cup \mathcal{R'}$.
\end{proof}
Derivability enjoys a number of structural properties that follow from its definition, independently of the rules $\mathcal{R}$ in question.

\begin{property}[Reflexivity]
    Every judgment is a consequence of itself:$\Gamma, J \vdash_\mathcal{R} J$.Each hypothesis justifies itself as conclusion.
\end{property}

\begin{property}[Weakening]
  If $\Gamma \vdash_\mathcal{R} J$,then $\Gamma, K \vdash_\mathcal{R} J$.Entailment  is  not  influenced  by  un-exercised options.
\end{property}

\begin{property}[Transitivity]
  If $\Gamma, K \vdash_\mathcal{R} J$ and $\Gamma \vdash_\mathcal{R} K$, then
  $\Gamma \vdash_\mathcal{R} J$. If we replace an axiom  by a derivation of it,
  the result is a derivation of its consequent without that  hypothesis
\end{property}

Reflexivity follows directly from the meaning of derivability. Weakening follows directly
from the definition of derivability. Transitivity is proved by rule induction on the first
premise

\section{Statics}
Most programming languages exhibit a \textit{phase distinction} between the
static and dynamic phases of processing. The static phase consists of parsing
and type checking to ensure that the program is well-formed; the dynamic phase
consists of execution of well-formed programs. A language is said to be
\textit{safe} exactly when well-formed programs are well-behaved when executed.

The static phase is specified by a \textit{statics} comprising a set of rules
for deriving \textit{typing judgments} stating that an expression is well-formed
of a certain type. Types mediate the interaction between the constituent parts
of a program by “predicting” some aspects of the execution behavior of the parts
so that we may ensure they fit together properly at run-time. Type safety tells
us that these predictions are correct; if not, the statics is considered to be
improperly defined, and the language is deemed \textit{unsafe} for execution.

\subsection{Syntax}
When defining a language we shall be primarily concerned with its abstract syntax,
specified by a collection of operators and their arities. The abstract syntax
provides a systematic, unambiguous account of the hierarchical and binding
structure of the language and is considered the official presentation of the language.
However, for the sake of clarity, it is also useful to specify minimal concrete syntax
conventions, without going through the trouble to set up a fully precise grammar for
it.
We will accomplish both of these purposes with a \textit{syntax chart}, whose meaning is
best illustrated by example. The following chart summarizes the abstract and concrete
syntax of.

\begin{tabular}{ccccc}
    $\mathbf{Typ} \ \tau$&~::= & $\mathbf{num}$ & $\mathbf{num}$ & numbers\\ 
    &  & $\mathbf{str}$ & $\mathbf{str}$ & strings\\ 
    $\mathbf{Exp} \ e$ &~::= & $x$ & $x$ & variable\\ 
    &  & $\mathbf{num}$[n] & n & numeral \\ 
    &  & $\mathbf{str}$[s] & ``s'' & literal\\ 
    &  & $\mathbf{plus}(e_1;e_2)$ & $e_1+e_2$ & addition\\ 
    &  & $\mathbf{times}(e_1;e_2$) & $e_1*e_2$ & multiplication\\ 
    &  & $\mathbf{cat}(e_1;e_2)$ & $e_1$\textasciicircum$ e_2$ & concatenation\\ 
    &  & $\mathbf{len}(e$) & $|e|$ & length\\ 
    &  & $\mathbf{let}(e_1;x.e_2$) & $\mathbf{let} \ x \ \mathbf{be}\  e_1 \textbf{in} \ e_2$ & definition\\ 
\end{tabular} \\


This chart defines two sorts,$\mathbf{Typ}$, ranged over by $\tau$, and
$\mathbf{Exp}$, ranged over by $e$. The chart defines a set of operators and
their arities. For example, it specifies that the operator $\mathbf{let}$ has
arity $\textbf{(Exp, Exp, Exp)}$, which specifies that it has two arguments of
sort $\textbf{Exp}$, and binds a variable of sort $\mathbf{Exp}$ in the second
argument.


\subsection{Type System} The role of a type system is to impose constraints on
the formations of phrases that are sensitive to the context in which they occur.
For example, whether the expression $\mathbf{plus}(x;\mathbf{num}[n])$ is
sensible depends on whether the variable $x$ is restricted to have type
$\mathbf{num}$ in the surrounding context of the expression. This example is, in
fact, illustrative of the general case, in that the only information required
about the context of an expression is the type of the variables within whose
scope the expression lies.  Consequently, the statics of $\mathbf{E}$ consists
of an inductive definition of generic hypothetical judgments of the form
\begin{center}
    $\overrightarrow{x}|\Gamma \vdash e:\tau$
\end{center}
where $\overrightarrow{x}$ is a finite set of variables, and $\Gamma$ is a
\textit{typing context} consisting of hypotheses of the form $x:\tau$, one for
each $x\in\overrightarrow{x}$. We rely on typographical conventions to determine
the set of variables, using the letters $x$ and $y$ to stand for them. We write
$x \notin \mathit{dom}(\Gamma)$ to say that ther is no assumption in $\Gamma$ of
the form $x:\tau$ for any type $\tau$, in which case we say that the variable
$x$ is \textit{fresh} for $\Gamma$.

The rules defining the statics of $\mathbf{E}$ are as follows:

\begin{mathpar}
	\inferrule*[Right=e\_var]{~}
    {\Gamma, x:\tau~\vdash~x:\tau}
\end{mathpar}
\begin{mathpar}
    \inferrule{~}
    {\Gamma~\vdash~\mathbf{str}[s]:\mathbf{str}}
\end{mathpar}
\begin{mathpar}
    \inferrule{~}
    {\Gamma~\vdash~\mathbf{num}[n]:\mathbf{num}}
\end{mathpar}

\begin{mathpar}
    \inferrule{\Gamma~\vdash~e_1: \mathbf{num} \ \Gamma~\vdash~e_2: \mathbf{num}}
    {\Gamma~\vdash~\mathbf{plus}~(e_1;e_2):\mathbf{num}}
\end{mathpar}

\begin{mathpar}
    \inferrule
    {\Gamma \vdash e_1: \mathbf{num} \ \Gamma \vdash e_2: \mathbf{num}}
    {\Gamma \vdash \mathbf{times}(e_1;e_2):\mathbf{num}}
\end{mathpar}

\begin{mathpar}
    \inferrule
    {\Gamma \vdash e_1: \mathbf{str} \ \Gamma \vdash e_2: \mathbf{str}}
    {\Gamma \vdash \mathbf{cat}(e_1;e_2):\mathbf{str}}
\end{mathpar}

\begin{mathpar}
    \inferrule
    {\Gamma \vdash e: \mathbf{str}}
    {\Gamma \vdash \mathbf{len}(e):\mathbf{str}}
\end{mathpar}

\begin{mathpar}
    \inferrule
    {\Gamma \vdash e_1: \tau_1 \ \Gamma, x:\tau_1 \vdash e_2: \tau_2}
    {\Gamma \vdash \mathbf{let}(e_1;x.e_2):\tau_2}
\end{mathpar}

In rule \texttt{E\_VAR}, we tacitly assume that the variable $x$ is not already declared in
$\Gamma$. This condition may always be met by choosing a suitable representative
of the $\alpha$-equivalence class of the let expression.

It is easy to check that every expression has at most one type by
\textit{induction on typing}, which is rule induction applied to rules. %??

\section{Type Safety}
FJ and FFJ are \textit{safe} (or, \textit{type safe}, or \textit{strongly
typed}). Informally, this means that certain kinds of mismatches cannot
arise during execution. For example, type safety for E states
that it will never arise that a number is to be added to a
string, or that two
numbers are to be concatenated, neither of which is meaningful.

This means that evaluation cannot get stuck in a state for which no
transition is possible, corresponding in implementation terms to the
absence of ``illegal instruction'' errors at execution time. This is proved
by showing that each step of transition can never ``go off into the
weeds'', and hence can never encounter an illegal instruction.


\section{Formal Semantics} 
In this article we are concearned in studying the formal semantics of
programming languages, i.e.\ we are interested in the tools to understand and
reason how programs behave. More specifically the semantics of Feature
Featherweight Java (FFJ).  For historical reasons the semantics of programming
languages is often viewed as consisteing of three strands: 

\begin{itemize} 
    \item \textit{Operational semantics} describes the meaning of the
        programming language by specifying how it executes on an abstract machine;

    \item \textit{Denotational semantics} is a technique for defining the
    meaning of programming languages. At one time called ``mathematical
    semantics'' it uses the more abstract mathematical concepts of complete
    partial orders, continous functions and least fixed points; 

    \item \textit{Axiomatic semantics} tries to fix the meaning of a programming
        contruct by giving proof rules for it whithin a program logic. Thus
        axiomatic semantics emphasises proof of correctness right from the start.
\end{itemize}

It would however be wrong to view these three styles as in opposition to
each other. They each have their uses. A clear operational semantics is very
helpful in implementation. Axiomatic semantics for special kinds of
languages can give strikingly elegant proof systems, useful in developing as
well as verifying programs. Denotational semantics provides the deepest and
most widely applicable thecniques, underpinned by a rich mathematical
theory. Indeed, the different styles  of semantics are highly dependent on
each other. 

For this we will need a nice understanding in logic and set theory.



\section{Featherweight Java}

%Como FFJ é construída com base em \textit{Featherweight Java} (FJ),
%introduziremos brevemente as principais características desta linguagem.
The constructions of FFJ are based on the constructions of \textit{Featherweight
Java} (FJ), therefore, we will briefly introduce the main characteristics of
this language.

\textit{Featherweight Java} was proposed by Igarash et.\ al., as a minimal core
calculus for modeling Java's type system~\cite{Igarashi99featherweightjava}. The 
design of this language favors compactness over completeness, having just five 
forms of expression:
\begin{itemize}
    \item object creation
    \item method invocation
    \item field access
    \item casting 
    \item variables
\end{itemize}

The purpose was to omit the maximum number of Java's characteristics while still
maintain a the main core, to ease the formalization model.
There is a direct correspondence between FJ and a purely functional core of
Java, in the sense that every FJ program is literally an executable Java
program.

%p: avoid using parenthesis, it is harder to understand 
%p: maybe we should consider explaining this after the example, then we can
%point to the constructor and everythings makes sense
Notice that FJ doesn't even allow assignment as a form of expression.
Assignment is used exclusively in field assignment inside the  constructors
using \texttt{this}. Thus all fields and method parameters are implicity marked
\texttt{final}. Therefore, FJ is restricted to a ``functional'' fragment of
Java. The authors state that it is easy to encode the lambda calculus to FJ to
prove that it is computationally complete.

%d: unecessary?
%p: I think it is ok
The compactness of FJ is important because the soundness proof becomes very
simple as the attention can be paid on essential aspects of the language, then
a rigorous soundness proof for even a significant extension of FJ remains
manageable. This is related to the authors's main goal: to make a proof of type
soundness (``well-typed programs do not get stuck'') as concise as possible, while
still capturing the essence of the soundness argument for the full Java language.

In FJ, a program consists of a collection of class definitions, later on defined
as the class table, and an expression to be evaluated. Here is an example of some typical
class definitions in FJ and a possible expression for this definitions.

\begin{lstlisting}[language=Java]
class A extends Object {
    A() { super(); } 
} 

class B extends Object { 
    B() { super(); }
} 

class Pair extends Object { 
    Object fst; 
    Object snd;
    Pair(Object fst, Object snd) { 
        super(); 
        this.fst = fst; 
        this.snd = snd; 
    } 
    Pair setfst(Object newfst) { 
        return new Pair(newfst, this.snd); 
    } 
} 
new Pair(new A(), new B()).setfst(new B())
\end{lstlisting}

\subsection{Syntax}

To present the abstract syntax of FJ, first we need to define the meaning of the
following metavariables, which will apear frequently in the rules of FJ's
grammar: \texttt{A, B, C, D} and \texttt{E} range over class names; \texttt{f}
and \texttt{g} range over field names; \texttt{m} ranges over method names;
\texttt{x} ranges over variables; \texttt{d} and \texttt{e} range over
expressions; \texttt{L} ranges over class declarations; \texttt{K} range over
constructor declarations and \texttt{M} ranges over method declarations.

The abstract syntax of FJ class declarations, constructor declarations, method
declarations and expressions is given at Table~\ref{abstractsyntax}.

%\begin{center}
\begin{table}[h!]
\begin{tabular}{ccl}
    $L$&~::= & $class\ C~extends~C\ \{\bar{C} \ \bar{f};\ K\
\bar{M}\}$\\ 
    \vspace{0.8mm}
    $K$&~::= &
$C~(\bar{C}~\bar{f})\
\{super~(\bar{f});~this.\bar{f}=\bar{f};\}$\\
    \vspace{0.8mm}
$M$&~::= & $C~m~(\bar{C}~\bar{x})\ \{return~e;\}$\\
    \vspace{0.8mm}
$e$&~::= & $x~|~e.f~|~e.m~(\bar{e})~|~new~C~(\bar{e})~|~(C)~e$ \\
\end{tabular} \\
\vspace{1.5mm}
\caption{Abstract Syntax}
\label{abstractsyntax}
\end{table}
%\end{center}

The variable \texttt{this} is assumed to be included in the set of variables, but
it cannot be used as the name of an argument to a method. This detail will be
important when we discuss the implementation details.
%The evaluation rule for method invocation will have the job of substituing an
%appropriate object for \texttt{this}, in addition to substituting the argument
%values for the parameters.

It is written $\bar{f}$ as a shorthand for a possibly empty sequence
$f_{1},\ldots,f_{n}$~(and similary for $\bar{C},\ \bar{x},\
\bar{e}$ etc.) and $\bar{M}$ as a shorthand for
$M_{1}\ldots M_{n}$~(with no commas).

A class table CT is a mapping from class names \texttt{C} to class declarations
\texttt{L}. A program is a pair (CT, \texttt{e}) of a class table and an
expression. In FJ, every class has a superclass declared with extends, with
the exception of \texttt{Object}, which is taken as a distinguished class name
whose definition does \textit{not} appear in the class table. The class table
contains the subtype relation between all the classes. From the
Table~\ref{subtyping}, one can infer that subtyping is the reflexive and
transitive closure of the immediate subclass ralation given by the
\texttt{extends} clauses in CT\@. 

\begin{table}[h!]
	\centering
	\begin{tabular}{c@{\hskip 1in}c@{\hskip 1in}c}
		$C~<:~C$ & 
		\inferrule{C <: D \qquad C <: E}
		{C <: E} &
		\inferrule{class~C~extends~D~\{~\ldots~\}}
		{C~<:~E} 
	\end{tabular}
\vspace{1.5mm}
\caption{Subtyping}
\label{subtyping}
\end{table}

The function $fields$ defined at Table~\ref{fieldlookup} returns a list of all
the fields declared at the current class definition, as well as the fields
declared at the superclass of it. It returns an empty list in the case of
\texttt{Object}.

\begin{table}[h!]
	\centering
	\def\arraystretch{2.5}
	\begin{tabular}{c}
		$fields~($\texttt{Object}$)=\bullet$ \\
		\inferrule{class\ C\ extends\ D~\{\bar{C}\ \bar{f};\ K\
		\bar{M}\} \qquad fields~(D)=\bar{D}\ \bar{g}}
		{fields~(C)=\bar{D}\ \bar{g},\ \bar{C}\ \bar{f}}
	\end{tabular}
\vspace{1.5mm}
\caption{Field lookup}
\label{fieldlookup}
\end{table}

The method declaration $D\ m~(\bar{C}\ \bar{x})\ \{return\ e;\}$
introduces a method name \texttt{m} with result type D and parameters
$\bar{x}$ of types $\bar{C}$. The body of the method is the single
statement \texttt{return e;}. The variables $\bar{x}$ and the special
variable \texttt{this} are bound in \texttt{e}. The rules that define these
functions are ilustrated in Tables~\ref{mtypelookup} and~\ref{mbodylookup}.

\begin{table}[h!]
	\centering
	\def\arraystretch{3}
	\begin{tabular}{c}
		\inferrule{class\ C\ extends\ D~\{\bar{C}\ \bar{f};\ K\
		\bar{M}\} \qquad B\ m~(\bar{B}\ \bar{x})\{return\
	e;\}\in~\bar{M}} {mtype~(m,~C)=\bar{B}\rightarrow~B} \\

		\inferrule{class\ C\ extends\ D~\{\bar{C}\ \bar{f};\ K\
		\bar{M}\} \qquad m\notin~\bar{M}}
		{mtype~(m,~C)=mtype~(m,~D)} \\
	\end{tabular}
\vspace{1.5mm}
\caption{Method type lookup}
\label{mtypelookup}
\end{table}

\begin{table}[h!]
	\centering
	\def\arraystretch{3}
	\begin{tabular}{c}
		\inferrule{class\ C\ extends\ D~\{\bar{C}\ \bar{f};\ K\
		\bar{M}\} \qquad B\ m~(\bar{B}\ \bar{x})\{return\
	e;\}\in~\bar{M}}
		{mbody~(m,~C)=\bar{x}.e} \\

		\inferrule{class\ C\ extends\ D~\{\bar{C}\ \bar{f};\ K\
		\bar{M}\} \qquad m\notin~\bar{M}}
		{mbody~(m,~C)=mbody~(m,~D)} \\
	\end{tabular}
\vspace{1.5mm}
\caption{Method body lookup}
\label{mbodylookup}
\end{table}

The given class table is assumed to satisfy some conditions:
\begin{itemize}
	\item $ CT~(C)=class\ C\ldots$ for every $C\in dom(CT)$
	\item \texttt{Object}$\notin dom(CT)$
	\item for every class name $C$~(except \texttt{Object}) appearing anywhere
		in CT, we have $C\in dom(CT)$
	\item there are no cycles in the subtype relation induced by CT, i.e., the
		relation <: is antisymmetric
\end{itemize}

\subsection{Typing}

The typing rules for expressions are in Table~\ref{exptyping}. An environment
$\Gamma$ is a finite mapping from variables to types, written $\bar{c}:\bar{C}$.
The typing judgment for expressions has the form $\Gamma \vdash e: C$, read ``in
the environment $\Gamma$, expression $e$ has type $C$''.

\begin{table}[h!]
	\centering
	\def\arraystretch{3}
	\begin{tabular}{cr}
		$\Gamma \vdash x:\Gamma(x)$& (T-Var)\\

		\inferrule{\Gamma \vdash e_{0}:C_{0}\qquad fields~(C_{0})=\bar{C}\
		\bar{f}}
		{\Gamma \vdash e_{0}.f_{i}:C_{i}} & (T-Field)\\

		\inferrule{\Gamma \vdash e_{0}:C_{0}\qquad
			mtypes~(m,~C_{0})=\bar{D}\rightarrow C\qquad \Gamma \vdash
		\bar{e} : \bar{C} \qquad \bar{C}~<:~\bar{D}}
		{\Gamma \vdash e_{0}.m(\bar{e}):C} & (T-Invk)\\

		\inferrule{fields(C)=\bar{D}\ \bar{f}\qquad \Gamma \vdash
		\bar{e}:\bar{C} \qquad \bar{C}~<:~\bar{D}}
		{\Gamma \vdash new\ C(\bar{e}):C} & (T-New)\\

		\inferrule{\Gamma \vdash e_{0}:D \qquad D~<:~C}
		{\Gamma \vdash (C)~e_{0}: C} & (T-UCast)\\

		\inferrule{\Gamma \vdash e_{0}:D\qquad C~<:~D \qquad C \neq D}
		{\Gamma \vdash (C)~e_{0}:C} & (T-DCast)\\

		\inferrule{\Gamma \vdash e_{0}:D\qquad C~\nless :~D \qquad D~\nless:~C 
		\qquad stupid\ warning}
		{\Gamma \vdash (C)~e_0:C} & (T-SCast)\\

	\end{tabular}
\vspace{1.5mm}
\caption{Expression typing}
\label{exptyping}
\end{table}
\section{Feature Featherweight Java}

This language extends FJ with constructions of \textit{feature-oriented
programming} (FOP) by new languages constructs for feature composition,
according evaluation and type rules. FFJ was proposed by Apel et.~al., in
\cite{Apel08featurefeatherweight}.

\textit{Feature-oriented programming} (FOP) aims at the modularization of
software systems in terms of features~\cite{fop}. A \textit{feature} implements
a stakeholder's requirement and is tipically an increment in program
functionality. Different variants of a software system are distinguished in
terms of their individual features.

%FFJ extende FJ com novas construções de linguagem orientados a composição de
%\textit{features}\cite{fop}, avaliação de acordos e regras de
%tipo~\cite{Apel08featurefeatherweight}. Em FFJ, um programador pode adicionar
%novas classes a um programa através da introdução de uma nova característica.

