\chapter{Theoretical Introduction}

%Quando a introducao estiver pronta adicionar aqui uma breve explciacao sobre a
%estrutura da introducao.

\section{Abstract Syntax}
Robert Harper in~\cite{practicalFoundations} defines Programming Languages
as means of expressing computations in a form comprehensible to
both people and machines and the syntax of a language specifies the means
by which various sorts of phrases (expressions, commands, declarations, and
so forth) may be combined to form programs.

To better understand the syntax, it is usually introduced a few other auxiliary
concepts, they are \textit{concrete syntax}, \textit{abstract syntax} and
\textit{binding}.
The \textit{surface}, or \textit{concrete syntax} is concerned with how
phrases are entered and displayed on a computer. The \textit{surface
syntax} is usually thought of as given by strings of characters from some
alphabet. 
The \textit{structural}, or \textit{abstract syntax} is concerned
with the structure of phrases, specifically how they are composed from
other phrases. 
At this level a phrase is a tree, called \textit{abstract
syntax tree}, whose nodes are operators that combine several phrases to
form another phrase. The \textit{binding} structure of syntax is concerned
with the introduction and use of identifiers how they are declared, and how
declared identifers are to be used. At  this level phrases are abstract
binding trees, which enrich \textit{abstract syntax} trees with the
concepts of binding and scope.

An \textit{abstract syntax tree}, or \textit{ast} for short, is an
ordered tree whose leaves are variables, and whose interior nodes are
operators whose arguments are its children. A \textit{variable} stands
for an unspecified, or generic piece of syntax of specified sort. Ast's
may be combined by an \textit{operator}, which as an \textit{arity}
specifying the sort of the operator and the number and sorts of its
arguments. 

A variable is an unknown object drawn from some domain. The unknown can
become known by substitution of a particular object for all occurrences of
a variable in a formula, thereby specializing a general formaula to a
particular instance. For example, in school, algebra variables range over
real numbers, say, 7 for x to obtain $7^2 + (2 \times 7) + 1$, which may be
simplified according to the laws of arithmetic to obtain 64, which is
indeed ${(7+1)}^2$ as it ought to be, since $x^2 + 2x + 1 = {(x + 1)}^2$ in
general, and hence in any particular case. The core idea is that \textit{a
variable is an unknown, or a placeholder, whose meaning is given by
susbtition}

As an example, consider a language of arithmetic expressions built from numbers
addition and multiplication. The abstract syntax of such a language consists of
a single sort, Exp, generated by these operations:

\begin{itemize}
    \item An operator num[n] of sort Exp for each n $\in$ N\@;
    \item Two operators, plus and times, of sort Exp, each with two
    arguments of sort Exp.
\end{itemize}

The expression $2+(3 * x)$, which involves a variable $x$, would represent by
the ast $plus(num[2]; times(num[3]; x))$, which is written informally as $2 +
(3 \times 4)$.  

\textit{Abstract binding trees}, or \textit{abt's} enrich ast's with
the means to introduce new variables and symbols, called a
\textit{binding}, with a specified range of significance, called its
\textit{scope}. The scope of a binding is an abt within which the bound
identifier may be used, either as a placeholder (in the case of a
variable declaration) or as the index of some operator (in the case of
a symbol declaration). Thus the set of active identifiers may be larger
within a subtree of an abt than it is within the surrounding tree.
Moreover, different subtrees may introduce identifiers with disjoint
scopes. The crucial principle is that any use of an identifier should
be understood as a reference, or abstract pointer, to its binding. One
consequence is that the choice of identifiers is immaterial, so long as
we can always associate a unique binding with each use of an
identifier.

For an example consider the expression $let x be a_1 in a_2$, which
introduces a variable $x$, for use within the expression $a_2$ to stand
for the expression $a_1$. The variable x is bound by the let expression
for use within $a_2$; any use of $x$ within $a_1$ refers to a different
variable that happens to have the same name. For example, in the
expression $let x be 7 in x+x$ ocurrences of x in the addition refer to
the variable introduced by the let. On the other and in the expression
$let be x*x in x+x$, occurrences of x within the multiplication refer
to a different variable than occurring whithin the addition.

\section{Inductive Definitions}
An inductive definition consists of a set of \textit{rules} for deriving
\textit{judgements}, or \textit{assertion}. Judgments are statements
about one or more abt's of some sort.  The rules specify necessary and
sufficient conditions for the validity of a judgement, and hence
determine its meaning.

As examples of \textit{judgements} we have:

\begin{table}[h!]
    \centering
    \caption{Judgements Example}
    \begin{tabular}{cc}
        $n$ nat & $n$ is a natural number \\
        $n$ = $n_1$ + $n_2$ & $n$ is the sum of $n_1$ and $n_2$\\
        t type & t is a type\\
        e~: t & expression e has type t\\
        e $ \Downarrow $ v & expression e has type v
    \end{tabular}
\end{table}

A judgment states that one or more abstract binding trees have a property
or stand in some relation to one another.

\section{Inference Rules}
An \textit{inductive definition} of a judgment form consists of a collection of
\textit{rules} of the form

\begin{mathpar}
\label{natrules1}
    \inferrule{J_1 \and \ldots \and J_k}
    {J}
\end{mathpar}

in which $J$ and $J_1, \cdots J_k$ are all judgments of the form
being defined. The judgments above the horizontal line are called
the \textit{premises} of the rule, and the judgement below the
line is called its \textit{conclusion}. If a rule has no premises
the rule is called an \textit{axiom}; otherwise it is called a
\textit{proper rule}.

For example, the following rules form an inductive definition of the 
judgement form --- nat:

\begin{mathpar}
\label{natrules2}
    \inferrule{~}
    {zero \ nat}
\end{mathpar}

\begin{mathpar} 
    \inferrule{a \ nat}
    {succ~(a) \ nat}
\end{mathpar}

These rules specify that a nat holds whenever either a is zero,
or a is succ~(b) where b nat for some b. Taking these rules to be
exhaustive, it follows that a nat iff a is a natural number.

\section{Derivation}
To show that an inductively defined judgement holds, it is enough to exhibit a
derivation of it. A derivation of a judgement is a finite composition of rules,
starting with axioms and ending with that judgment. It may be thought of as a
tree in which each node is a rule whose children are derivations of its
premises. We sometimes say that a derivation of J is evidence for the validity
of an inductively defined judgement J.

We usually depict derivations as trees with the conclusion at the bottom, and
with the children of a node corresponding to a rule appearing above it as
evidence for the premises of that rule.

For example, this is a derivation of succ~(succ~(succ~(zero))) nat:

\begin{mathpar}
\label{natrules}
    \inferrule*
      {\inferrule* 
        {\inferrule* 
          {\inferrule* {~} {zero \ nat}}
        {succ~(zero)}}
      {succ~(succ~(zero))}}
    {succ~(succ~(succ~(zero)))}
\end{mathpar}~

To show that an inductively defined judgment is derivable, we need only find a
derivation for it. There are two main methods for finding derivations, called
\textit{forward chaining}, or \textit{bottom-up construction}, and
\textit{backward chaining}, or \textit{top-down construction}.
Forward chaining starts with the axioms and works forward towards the desired
conclusion, whereas backward chaining starts with the desired conclusion and
works backwards towards the axioms.

More precisely, forward chaining search maintains a set of derivable judgments and
continually extends this set by adding to it the conclusion of any rule all of whose
premises are in that set. Initially, the set is empty; the process terminates when the
desired judgment occurs in the set. Assuming that all rules are considered at every stage,
forward chaining will eventually find a derivation of any derivable judgment, but it is
impossible (in general) to decide algorithmically when to stop extending the set and
conclude that the desired judgment is not derivable. We may go on and on adding more
judgments to the derivable set without ever achieving the intended goal. It is a matter of
understanding the global properties of the rules to determine that a given judgment is
not derivable.


Forward chaining is undirected in the sense that it does not take account of the end
goal when deciding how to proceed at each step. In contrast, backward chaining is
goal-directed. Backward chaining search maintains a queue of current goals,
judgments whose derivations are to be sought. Initially, this set consists solely of the
judgment we wish to derive. At each stage, we remove a judgment from the queue
and consider all rules whose conclusion is that judgment. For each such rule,
we add the premises of that rule to the back of the queue, and continue. If
there is more than one such rule, this process must be repeated, with the same
starting queue, for each candidate rule. The process terminates whenever the
queue is empty, all goals having been achieved; any pending consideration of
candidate rules along the way can be discarded. As with forward chaining,
backward chaining will eventually find a derivation of any derivable judgment, but
there is, in general, no algorithmic method for determining in general whether
the current goal is derivable. If it is not, we may futilely add more and more
judgments to the goal set, never reaching a point at which all goals have been
satisfied.


\section{Rule Induction}

Because an inductive definition specifies the \textit{strongest} judgment form closed under a
collection of rules, we may reason about them by \textit{rule induction}. The principle of rule
induction states that to show that a property $a\ \mathcal{P}$ holds whenever $a
\ \mathsf{J}$ is derivable, it is enough to show that $\mathcal{P}$ is
\textit{closed under}, or \textit{respects}, the rules defining the judgment
form $\mathsf{J}$. More precisely, the property $\mathcal{P}$ respects the rule

\begin{mathpar} 
    \inferrule
    {a_1 \mathsf{J} \ \ldots \ a_k \mathsf{J}}
    {a \mathsf{J}}
\end{mathpar}
if $\mathcal{P}(a)$ holds whenever $\mathcal{P}(a_1),\ldots,\mathcal{P}(a_k)$ do.
The assumptions $\mathcal{P}(a_1),\ldots,\mathcal{P}(a_k)$ are called the
\textit{inductive hypotheses}, and $\mathcal{P}(a)$ is called \textit{inductive
conclusion} of the inference.

The principle of rule induction is simply the expression of the definition of an
inductively defined judgment form as the \textit{strongest} judgment form closed
under the rules comprising the definition. Thus, the judgment form defined by a
set of rules is both (a) closed under those rules, and (b) sufficient for any
other property also closed under those rules. The former means that a derivation
is evidence for the validity of a judgment; the
latter means that we may reason about an inductively defined judgment form by rule induction

When specialized to the nat rules, the principle of rule induction states that
to show $\mathcal{P}(a)$ whenver $a \ nat$, it is enough to show:

\begin{enumerate}
        \item $P(zero)$
        \item for every $a$, if $\mathcal{P}(a)$, then $\mathcal{P}(succ(a))$.
\end{enumerate}
The sufficiency of these conditions is the familiar principle of \textit{mathematical induction}

\section{Hypothetical Judgements}
A \textit{hypothetical judgment} expresses an entailment between one or more
hypotheses and a conclusion. We will consider two notions of entailment, called
\textit{derivability} and \textit{admissibility}. Both express a form of
entailment, but they differ in that derivability is stable under extension with
new rules, admissibility is not. A \textit{general judgment} expresses
the universality, or genericity, of a judgment. There are two forms of general
judgment, the \textit{generic} and the \textit{parametric}. The generic judgment
expresses generality with respect to all substitution instances for variables in
a judgment. The parametric judgment expresses generality with respect to
renamings of symbols.

The hypothetical judgment codifies the rules for expressing the validity of a
conclusion conditional on the validity of one or more hypotheses. There are two
forms of hypothetical judgment that differ according to the sense in which the
conclusion is conditional on the hypotheses. One is stable under extension with
more rules, and the other is not.

\subsection{Derivability}
For a given set $\mathcal{R}$ of rules, we define the \textit{derivability}
judgement, written $J_1,\ldots,J_k \vdash_\mathcal{R} K$, where each $J_i$ and $K$
are basic judgements, to mean that we may derive $K$ from the \textit{expansion}
of the rules $\mathcal{R}$ with the axioms

\begin{mathpar}
            \inferrule{~} {J_1} \quad
            \ldots \quad
            \inferrule{~} {J_k} \quad
\end{mathpar}

We treat the \textit{hypotheses}, or \textit{antecedents}, of the judgment $J_1,
\ldots, J_k$ as “temporary axioms,” and derive the conclusion, or consequent, by
composing rules in $\mathcal{R}$. Thus, evidence for a hypothetical judgment
consists of a derivation of the conclusion from the
hypotheses using the rules in $\mathcal{R}$.
We use capital Greek letters, usually $\Gamma$ or $\Delta$, to stand for a
finite set of basic judgments, and write $\mathcal{R} \bigcup \Gamma$ for the
expansion of $\mathcal{R}$ with an axiom corresponding to each judgment in
$\Gamma$.The judgment $\Gamma \vdash_\mathcal{R} K$ means that $K$ is
derivable from rules $\mathcal{R} \bigcup \Gamma$, and the judgment
$\vdash_\mathcal{R} \Gamma$ means that $\vdash_\mathcal{R} J$ for each in
$\Gamma$. An equivalent way of defining $J_1, \ldots, J_n \vdash_\mathcal{R} J$ is
to say that the rule
\begin{mathpar}
    \inferrule{J_1 \quad \ldots \quad J_n} {J}
\end{mathpar}

is \textit{derivable} from $mathcal{R}$, which means that there is a derivation
of $J$ composed of the rules in $\mathcal{R}$ augmented by treating $J_1, \ldots,
J_n$ as axioms.

For example, consider the derivability judgment.
\begin{displaymath}
    a \ nat \vdash succ(succ(a)) nat
\end{displaymath}
This judgement is valid for any choice of object $a$, as shown by the derivation

\begin{mathpar}
    \inferrule* 
        {\inferrule* 
            {\inferrule* {~} {a \ nat}}
            {succ~(a)}}
        {succ~(succ~(a))}
\end{mathpar}

which composes rules~??, starting with $a \ nat$ as an axiom, and ending with
$succ~(succ~(a)) nat$.

It follows directly from the definition of derivability that it is stable under
extension with new rules.

\begin{theorem}[Stability]
    If $\Gamma \vdash_\mathcal{R} J$, then $\Gamma \vdash_{\mathcal{R} \cup \mathcal{R'}} J$
\end{theorem}

\begin{proof}
    Any derivation of $J$ from $\mathcal{R} \bigcup \Gamma$ is also a derivation
    from $(\mathcal{R} \cup \mathcal{R'})\cup \Gamma$, because any rule in
    $\mathcal{R}$ is also a rule in $\mathcal{R} \cup \mathcal{R'}$.
\end{proof}
Derivability enjoys a number of structural properties that follow from its definition, independently of the rules $\mathcal{R}$ in question.

\begin{property}[Reflexivity]
    Every judgment is a consequence of itself:$\Gamma, J \vdash_\mathcal{R} J$.Each hypothesis justifies itself as conclusion.
\end{property}

\begin{property}[Weakening]
  If $\Gamma \vdash_\mathcal{R} J$,then $\Gamma, K \vdash_\mathcal{R} J$.Entailment  is  not  influenced  by  un-exercised options.
\end{property}

\begin{property}[Transitivity]
  If $\Gamma, K \vdash_\mathcal{R} J$ and $\Gamma \vdash_\mathcal{R} K$, then
  $\Gamma \vdash_\mathcal{R} J$. If we replace an axiom  by a derivation of it,
  the result is a derivation of its consequent without that  hypothesis
\end{property}

Reflexivity follows directly from the meaning of derivability. Weakening follows directly
from the definition of derivability. Transitivity is proved by rule induction on the first
premise

\section{Statics}
Most programming languages exhibit a \textit{phase distinction} between the
static and dynamic phases of processing. The static phase consists of parsing
and type checking to ensure that the program is well-formed; the dynamic phase
consists of execution of well-formed programs. A language is said to be
\textit{safe} exactly when well-formed programs are well-behaved when executed.

The static phase is specified by a \textit{statics} comprising a set of rules
for deriving \textit{typing judgments} stating that an expression is well-formed
of a certain type. Types mediate the interaction between the constituent parts
of a program by “predicting” some aspects of the execution behavior of the parts
so that we may ensure they fit together properly at run-time. Type safety tells
us that these predictions are correct; if not, the statics is considered to be
improperly defined, and the language is deemed \textit{unsafe} for execution.

\subsection{Syntax}
When defining a language, we shall be primarily concerned with its abstract syntax,
specified by a collection of operators and their arities. The abstract syntax
provides a systematic, unambiguous account of the hierarchical and binding
structure of the language and is considered the official presentation of the language.
However, for the sake of clarity, it is also useful to specify minimal concrete syntax
conventions, without going through the trouble to set up a fully precise grammar for
it.
We will accomplish both of these purposes with a \textit{syntax chart}, whose meaning is
best illustrated by example. The following chart summarizes the abstract and concrete
syntax of.

\begin{tabular}{ccccc}
    $\mathbf{Typ} \ \tau$&~::= & $\mathbf{num}$ & $\mathbf{num}$ & numbers\\ 
    &  & $\mathbf{str}$ & $\mathbf{str}$ & strings\\ 
    $\mathbf{Exp} \ e$ &~::= & $x$ & $x$ & variable\\ 
    &  & $\mathbf{num}$[n] & n & numeral \\ 
    &  & $\mathbf{str}$[s] & ``s'' & literal\\ 
    &  & $\mathbf{plus}(e_1;e_2)$ & $e_1+e_2$ & addition\\ 
    &  & $\mathbf{times}(e_1;e_2$) & $e_1*e_2$ & multiplication\\ 
    &  & $\mathbf{cat}(e_1;e_2)$ & $e_1$\textasciicircum$ e_2$ & concatenation\\ 
    &  & $\mathbf{len}(e$) & $|e|$ & length\\ 
    &  & $\mathbf{let}(e_1;x.e_2$) & $\mathbf{let} \ x \ \mathbf{be}\  e_1 \textbf{in} \ e_2$ & definition\\ 
\end{tabular} \\


This chart defines two sorts,$\mathbf{Typ}$, ranged over by $\tau$, and
$\mathbf{Exp}$, ranged over by $e$. The chart defines a set of operators and
their arities. For example, it specifies that the operator $\mathbf{let}$ has
arity $\textbf{(Exp, Exp, Exp)}$, which specifies that it has two arguments of
sort $\textbf{Exp}$, and binds a variable of sort $\mathbf{Exp}$ in the second
argument.


\subsection{Type System} The role of a type system is to impose constraints on
the formations of phrases that are sensitive to the context in which they occur.
For example, whether the expression $\mathbf{plus}(x;\mathbf{num}[n])$ is
sensible depends on whether the variable $x$ is restricted to have type
$\mathbf{num}$ in the surrounding context of the expression. This example is, in
fact, illustrative of the general case, in that the only information required
about the context of an expression is the type of the variables within whose
scope the expression lies.  Consequently, the statics of $\mathbf{E}$ consists
of an inductive definition of generic hypothetical judgments of the form
\begin{center}
    $\overrightarrow{x}|\Gamma \vdash e:\tau$
\end{center}
where $\overrightarrow{x}$ is a finite set of variables, and $\Gamma$ is a
\textit{typing context} consisting of hypotheses of the form $x:\tau$, one for
each $x\in\overrightarrow{x}$. We rely on typographical conventions to determine
the set of variables, using the letters $x$ and $y$ to stand for them. We write
$x \notin \mathit{dom}(\Gamma)$ to say that ther is no assumption in $\Gamma$ of
the form $x:\tau$ for any type $\tau$, in which case we say that the variable
$x$ is \textit{fresh} for $\Gamma$.

The rules defining the statics of $\mathbf{E}$ are as follows:

\begin{mathpar}
	\inferrule*[Right=e\_var]{~}
    {\Gamma, x:\tau~\vdash~x:\tau}
\end{mathpar}
\begin{mathpar}
    \inferrule{~}
    {\Gamma~\vdash~\mathbf{str}[s]:\mathbf{str}}
\end{mathpar}
\begin{mathpar}
    \inferrule{~}
    {\Gamma~\vdash~\mathbf{num}[n]:\mathbf{num}}
\end{mathpar}

\begin{mathpar}
    \inferrule{\Gamma~\vdash~e_1: \mathbf{num} \ \Gamma~\vdash~e_2: \mathbf{num}}
    {\Gamma~\vdash~\mathbf{plus}~(e_1;e_2):\mathbf{num}}
\end{mathpar}

\begin{mathpar}
    \inferrule
    {\Gamma \vdash e_1: \mathbf{num} \ \Gamma \vdash e_2: \mathbf{num}}
    {\Gamma \vdash \mathbf{times}(e_1;e_2):\mathbf{num}}
\end{mathpar}

\begin{mathpar}
    \inferrule
    {\Gamma \vdash e_1: \mathbf{str} \ \Gamma \vdash e_2: \mathbf{str}}
    {\Gamma \vdash \mathbf{cat}(e_1;e_2):\mathbf{str}}
\end{mathpar}

\begin{mathpar}
    \inferrule
    {\Gamma \vdash e: \mathbf{str}}
    {\Gamma \vdash \mathbf{len}(e):\mathbf{str}}
\end{mathpar}

\begin{mathpar}
    \inferrule
    {\Gamma \vdash e_1: \tau_1 \ \Gamma, x:\tau_1 \vdash e_2: \tau_2}
    {\Gamma \vdash \mathbf{let}(e_1;x.e_2):\tau_2}
\end{mathpar}

In rule \texttt{E\_VAR}, we tacitly assume that the variable $x$ is not already declared in
$\Gamma$. This condition may always be met by choosing a suitable representative
of the $\alpha$-equivalence class of the let expression.

It is easy to check that every expression has at most one type by
\textit{induction on typing}, which is rule induction applied to rules. %??

\section{Type Safety}
FJ and FFJ are \textit{safe} (or, \textit{type safe}, or \textit{strongly
typed}). Informally, this means that certain kinds of mismatches cannot
arise during execution. For example, type safety for E states
that it will never arise that a number is to be added to a
string, or that two
numbers are to be concatenated, neither of which is meaningful.

This means that evaluation cannot get stuck in a state for which no
transition is possible, corresponding in implementation terms to the
absence of ``illegal instruction'' errors at execution time. This is proved
by showing that each step of transition can never ``go off into the
weeds'', and hence can never encounter an illegal instruction.


\section{Formal Semantics} 
In this article we are concearned in studying the formal semantics of
programming languages, i.e.\ we are interested in the tools to understand and
reason how programs behave. More specifically the semantics of Feature
Featherweight Java (FFJ).  For historical reasons the semantics of programming
languages is often viewed as consisteing of three strands: 

\begin{itemize} 
    \item \textit{Operational semantics} describes the meaning of the
        programming language by specifying how it executes on an abstract machine;

    \item \textit{Denotational semantics} is a technique for defining the
    meaning of programming languages. At one time called ``mathematical
    semantics'' it uses the more abstract mathematical concepts of complete
    partial orders, continous functions and least fixed points; 

    \item \textit{Axiomatic semantics} tries to fix the meaning of a programming
        contruct by giving proof rules for it whithin a program logic. Thus
        axiomatic semantics emphasises proof of correctness right from the start.
\end{itemize}

It would however be wrong to view these three styles as in opposition to
each other. They each have their uses. A clear operational semantics is very
helpful in implementation. Axiomatic semantics for special kinds of
languages can give strikingly elegant proof systems, useful in developing as
well as verifying programs. Denotational semantics provides the deepest and
most widely applicable thecniques, underpinned by a rich mathematical
theory. Indeed, the different styles  of semantics are highly dependent on
each other. 

For this we will need a nice understanding in logic and set theory.



\section{Featherweight Java}

%Como FFJ é construída com base em \textit{Featherweight Java} (FJ),
%introduziremos brevemente as principais características desta linguagem.
The constructions of FFJ are based on the constructions of \textit{Featherweight
Java} (FJ), therefore, we must furst introduce the main characteristics of
this language.

\textit{Featherweight Java} was proposed by Igarash et.\ al., as a minimal core
calculus for modeling Java's type system~\cite{Igarashi99featherweightjava}. The 
design of this language favors compactness over completeness, having just five 
forms of expression:
\begin{itemize}
    \item Object creation;
    \item Method invocation;
    \item Field access;
    \item Casting;
    \item Variables
\end{itemize}

Several studies have introduced lightweight versions of Java
The purpose is to omit the maximum number of Java's characteristics while
still maintaining the main core, to ease the formalization model. There is a
direct correspondence between FJ and a purely functional core of Java, in the
sense that every FJ program is literally an executable Java program.

The design of FJ favors compactness over completeness almost obsessivelly.  The
aim is to omit as many features as possible, even assignment, while retaining
the core features of Java typing.  Assignment is used exclusively in field
assignment inside the  constructors using \texttt{this}. 

FJ is only a little larger than Church's lambda calculus~\cite{barendregt1984}
or abadi and Cardelli's object calculus~\cite{abadi1996}, and is significantly
smaller than other precious formal models of class-based languages. 
Being smaller, FJ lets us focus on just a few key issues.

FJ has been successfully used to study Java extentions because it is so
compact, we can focus attention on essential aspectos of the extension.
Moreover, because the proof of soundness for pure FJ is very simple, a rigorous
soundness proof for even a significant extension may remain manageable.  The
compactness of FJ is important because the soundness proof becomes very simple
as the attention can be paid on essential aspects of the language, then a
rigorous soundness proof for even a significant extension of FJ remains
manageable. This is related to the authors's main goal: to make a proof of type
soundness (``well-typed programs do not get stuck'') as concise as possible,
while still capturing the essence of the soundness argument for the full Java
language.

The Java features omitted from FJ include assignment, interfaces, overloading,
messages to \texttt{super}, \textit{null} pointers, base types (\texttt{int},
\texttt{bool}, etc.), abstract method declarations, shadowing of superclass
fields by subclass fields, access control (\texttt{public}, \texttt{private},
etc.), and exceptions. The features of Java that it does model include mutually
recursive class definitions, method recursion through \texttt{this}, subtyping,
and casting.

One key simplification in FJ is the omission of assignment. In essence, all
fields and method parameters in FJ are implicitly marked \texttt{final}: it is
assumed that an object's field are initialized by its constructor and never
changed aftward. This restricts FJ to a ``functional'' fragment of Java, in
which many common Java idioms, such as use of enumerations, cannot be
represented. Nonetheless, this fragment is computationally complete (it is easy
to encode the lambda calculus into it), and is large enough to include many
useful programs like programs in Felleisen and Friedman's Java
text~\cite{felleisen1998} which use a purely functional style. Moreover, most
of the tricky typing issues in Java are independent of assignment. 

\subsection{FJ explanation}
%d: unecessary?
%p: I think it is ok
In FJ, a program consists of a collection of class definitions, later on
defined as the class table, and an expression to be evaluated. This expression
correspods to the body of the \texttt{main} method in full Java. Here is an
example of some typical class definitions in FJ and a possible expression for
this definitions.

\begin{lstlisting}[language=Java]
    class A extends Object {
        A() { super(); } 
    } 

    class B extends Object { 
        B() { super(); }
    } 

    class Pair extends Object { 
        Object fst; 
        Object snd;
        Pair(Object fst, Object snd) { 
            super(); 
            this.fst = fst; 
            this.snd = snd; 
        } 
        Pair setfst(Object newfst) { 
            return new Pair(newfst, this.snd); 
        } 
    } 
    new Pair(new A(), new B()).setfst(new B())
\end{lstlisting}

We always
\begin{enumerate}
    \item Include the supertype (even when it is \texttt{Object});
    \item Write out the constructor (even for the trivial classes \texttt{A} and \texttt{B}; 
    \item Write the receiver for the field access (as in \texttt{this.snd}) or a method invocation, even when the receiver is \texttt{this}
\end{enumerate}
Constructor always take the same stylized form: there is one parameter for each
field, with the same name as the field; the \texttt{super} constructor is
invoked on the fields of the supertype; and the remaining fields are
initialized to the corresponding parameters. In this example the supertype is
always \texttt{Object}, which has no fields, so the invocations of
\texttt{super} have no arguments. Constructors are the only place where
\texttt{Super} or = appears in an FJ program. Since FJ provides no
side-effecting operations, a method body always consists of return followed by
an expression, as in the body of \texttt{setfst()}.

In the context of the above definitions, the expression
\begin{alltt}
    \begin{lstlisting}[language=Java]
        new Pair(new A(), new B()).setfst(new B())
    \end{lstlisting}
\end{alltt}
evaluates to the expression
\begin{alltt}
    \begin{lstlisting}[language=Java]
        new Pair(new B(), new B())
    \end{lstlisting}
\end{alltt}

There are five forms of expression in FJ
\begin{enumerate}
\item Object constructors, like \texttt{new A()}, \texttt{new B()} and \texttt{new Pair(e1, e2)};
\item Method invocation, like \texttt{e3.setfst(e4)};
\item Field access, like \texttt{this.snd} inside the body of \texttt{setfst};
\item Variabes, like the occurencies of \texttt{newfst} and \texttt{this};
\item Cast, like the expression %
\begin{lstlisting}[language=Java]
((Pair)new Pair(new Pair(new A(), newB()), new A()).fst).snd 
\end{lstlisting}
which evaluates to
\begin{center}
\begin{tabular}{c}
\begin{lstlisting}[language=Java]
new B()
\end{lstlisting}
\end{tabular}{c}
\end{center}
\end{enumerate} 

In Java, we may prefix a field or parameter declaration with the keyword \texttt{final}
to indicate that it cannot be reassigned. However, since FJ has no assignments, there is
no need for such a keyword.

Given that FJ functions has no side effects evaluation gets a lot easier to be formalized.
Since it can be interely done within the syntax and no need for a heap mechanism whatsoever.
With the absence of side effects, the order in which expressions are evaluated does not affect
the final outcome of the computation (except when the program does not terminates).

There are three basic computation rules: one for field access, one for method
invocation, and one for casts.  It is recursive with the stop condition on the
\texttt{new} expression. That is to say ``everything is an object'' here in FJ.

The following example shows the rule for field access in action.

\begin{center}
\begin{tabular}{c}
\begin{lstlisting}[language=Java, mathescape = true]
    new Pair(new A(), newB()).snd ${\rightarrow}$ new B()
\end{lstlisting}
\end{tabular}
\end{center}

Bringing into attention the object constructors, it has one parameter for each
field, in the same order that the fields are declared, also taking into account
its superclasses.  Invocation of methods reduces to the body with the formal
parameter replaced by the actual, and the special variable \texttt{this}
replaced by the receiver object. This is similar to the beta rula of the lambda
calculus. The key differences are the fact that the class of the receiver
determines where to look for the ody (supporting method override), and the
substitution of the receiver for \texttt{this} (suporting method override), and
the substitution of the receiver for \texttt{this}(supporting ``recursion
through self''). In FJ, as in the lambda calculus and the pure Abadi-Cardeli
calculus, if a formal parameter appears more than once in the body it may lead
to duplication of the actual, but since there are no side effects this causes
no problems.

Here is an example of a reduction of a cast:
\begin{center}
\begin{tabular}{c}
\begin{lstlisting}[language=Java, mathescape = true]
(Pair)new Pair(new A(), new B()) ${\rightarrow}$ new Pair(new A(), new B())
\end{lstlisting}
\end{tabular}
\end{center}

When a cast-expression is reduced to an object (i.e. a new-expression), it is
easy to check that the class of the constructor is a subclass of the target of
the cast. If so, as in this case, then the reduction removes the cast.
Otherwise, is in \texttt{(A)new B()}, then no rule applies and the computation
is \textit{stuck}, raising a runtime error.

There are three ways computation may get stuck: an attempt to access a field
not declared for the class; an attempt to invoke a method not declared for the
class; or an attempt to cast to something other than superclass of an object's
runtime class. The authors provides a proof that the first two never happen in
well-typed programs, and the third never happens in well-typed programs that
contain no downcasts (and no ``stupid casts'', which is explained in details
below).

As usual, it is allowed to apply reductions to any subexpressions of an
expressions. Below is provided a more complex example, underlining the step of
the reduction being applied.
\begin{lstlisting}[language=Java, mathescape = true, escapechar = \*]
   (Pair)*\underline{new Pair(new Pair(new A(), new B()), new A()).fst}*).snd
$\rightarrow$ *\underline{(Pair)new Pair(new A(), new B())}*.snd
$\rightarrow$ *\underline{new Pair(new A(), new B()).snd}*
$\rightarrow$ new B()
\end{lstlisting}

In the original paper~\cite{Igarashi99featherweightjava} it is provided a proof
of a type soundness result for FJ: if a well-typed expression \texttt{e}
reduces to a normal form, an expression that cannot reduce any further, than
the normal form is either a well-typepd value (an expression consisting only of
\texttt{new}), whose type is a subtype of the type of \texttt{e}, or stuck at a
failing typecast.

Finally we present the following code with the implementation in FJ of the
Natural numbers using only the constructor for zero and the successor, known as
the Peano Numbers~\cite{Peano1889}

\iffalse
\noident
\begin{lstlisting}[language=Java]
class Nat extends Object{
    Nat(){super();}

    Nat add(Nat rhs){
        return rhs.add(this);
    }
}

class O extends Nat{
    O(){super();}

    Nat add(Nat rhs){
        return rhs;
    }
}

class S extends Nat{
    Nat num;

    S(Nat num){
        super();
        this.num=num;
    }

    Nat add(Nat rhs){
        return this.num.add(new S(rhs));
    }
    
}
new S(new S(new O())).add(new S(new O()))
\end{lstsliting}
\fi
With this informal introduction we may now proceed to the formal definitions.

\subsection{Syntax}

To present the abstract syntax of FJ, first we need to define the meaning of the
following metavariables, which will apear frequently in the rules of FJ's
grammar: ${A, B, C, D}$ and ${E}$ range over class names; $f$
and $g$ range over field names; $m$ ranges over method names;
$x$ ranges over variables; $d$ and $e$ range over
expressions; $L$ ranges over class declarations; $K$ range over
constructor declarations and $M$ ranges over method declarations.

The abstract syntax of FJ class declarations, constructor declarations, method
declarations and expressions is given at Table~\ref{abstractsyntax}.

%\begin{center}
\begin{table}[ht!]
    \caption{Abstract Syntax}
    \begin{tabular}{ccl}
            $L$&~::= & $class\ C~extends~C\ \{\bar{C} \ \bar{f};\ K\
        \bar{M}\}$\\ 
            \vspace{0.8mm}
            $K$&~::= &
        $C~(\bar{C}~\bar{f})\
        \{super~(\bar{f});~this.\bar{f}=\bar{f};\}$\\
            \vspace{0.8mm}
        $M$&~::= & $C~m~(\bar{C}~\bar{x})\ \{return~e;\}$\\
            \vspace{0.8mm}
        $e$&~::= & $x~|~e.f~|~e.m~(\bar{e})~|~new~C~(\bar{e})~|~(C)~e$ \\
    \end{tabular} \\
    \vspace{1.5mm}
    \label{abstractsyntax}
\end{table}
%\end{center}

The variable \texttt{this} is assumed to be included in the set of variables, but
it cannot be used as the name of an argument to a method. 
%The evaluation rule for method invocation will have the job of substituing an
%appropriate object for \texttt{this}, in addition to substituting the argument
%values for the parameters.

It is written $\bar{f}$ as a shorthand for a possibly empty sequence
$f_{1},\ldots,f_{n}$~(and similary for $\bar{C},\ \bar{x},\
\bar{e}$ etc.) and $\bar{M}$ as a shorthand for
$M_{1}\ldots M_{n}$~(with no commas).

A class table CT is a mapping from class names $C$ to class declarations
$L$. A program is a pair (CT, $e$) of a class table and an
expression. In FJ, every class has a superclass declared with extends, with
the exception of \texttt{Object}, which is taken as a distinguished class name
whose definition does \textit{not} appear in the class table. The class table
contains the subtype relation between all the classes. From the
Table~\ref{subtyping}, one can infer that subtyping is the reflexive and
transitive closure of the immediate subclass ralation given by the
\texttt{extends} clauses in CT\@. 

\begin{table}[ht!]
	\centering
    \caption{Subtyping}
    \label{subtyping}
	\begin{tabular}{c@{\hskip 1in}c@{\hskip 1in}c}
		$C~<:~C$ & 
		\inferrule{C <: D \qquad C <: E}
		{C <: E} &
		\inferrule{class~C~extends~D~\{~\ldots~\}}
		{C~<:~E} 
	\end{tabular}
\vspace{1.5mm}
\end{table}

The function $fields$ defined at Table~\ref{fieldlookup} returns a list of all
the fields declared at the current class definition, as well as the fields
declared at the superclass of it. It returns an empty list in the case of
\texttt{Object}.

\begin{table}[ht!]
	\centering
    \caption{Field lookup}
    \label{fieldlookup}
	\def\arraystretch{2.5}
	\begin{tabular}{c}
		$fields~($\texttt{Object}$)=\bullet$ \\
		\inferrule{class\ C\ extends\ D~\{\bar{C}\ \bar{f};\ K\
		\bar{M}\} \qquad fields~(D)=\bar{D}\ \bar{g}}
		{fields~(C)=\bar{D}\ \bar{g},\ \bar{C}\ \bar{f}}
	\end{tabular}
\vspace{1.5mm}
\end{table}

The method declaration $D\ m~(\bar{C}\ \bar{x})\ \{return\ e;\}$
introduces a method name $m$ with result type D and parameters
$\bar{x}$ of types $\bar{C}$. The body of the method is the single
statement $return e;$. The variables ${x}$ and the special
variable ${this}$ are bound in ${e}$. The rules that define these
functions are ilustrated in Tables~\ref{mtypelookup} and~\ref{mbodylookup}.

\begin{table}[h!]
	\centering
	\def\arraystretch{3}
    \caption{Method type lookup}
    \label{mtypelookup}
	\begin{tabular}{c}
		\inferrule{class\ C\ extends\ D~\{\bar{C}\ \bar{f};\ K\
		\bar{M}\} \qquad B\ m~(\bar{B}\ \bar{x})\{return\
	e;\}\in~\bar{M}} {mtype~(m,~C)=\bar{B}\rightarrow~B} \\

		\inferrule{class\ C\ extends\ D~\{\bar{C}\ \bar{f};\ K\
		\bar{M}\} \qquad m\notin~\bar{M}}
		{mtype~(m,~C)=mtype~(m,~D)} \\
	\end{tabular}
\vspace{1.5mm}
\end{table}

\begin{table}[h!]
	\centering
    \caption{Method body lookup}
	\def\arraystretch{3}
    \label{mbodylookup}
	\begin{tabular}{c}
		\inferrule{class\ C\ extends\ D~\{\bar{C}\ \bar{f};\ K\
		\bar{M}\} \qquad B\ m~(\bar{B}\ \bar{x})\{return\
	e;\}\in~\bar{M}}
		{mbody~(m,~C)=\bar{x}.e} \\

		\inferrule{class\ C\ extends\ D~\{\bar{C}\ \bar{f};\ K\
		\bar{M}\} \qquad m\notin~\bar{M}}
		{mbody~(m,~C)=mbody~(m,~D)} \\
	\end{tabular}
\vspace{1.5mm}
\end{table}

The given class table is assumed to satisfy the following conditions:
\begin{itemize}
	\item $ CT~(C)=class\ C\ldots$ for every $C\in dom(CT)$
	\item \texttt{Object}$\notin dom(CT)$
	\item for every class name $C$~(except \texttt{Object}) appearing anywhere
		in CT, we have $C\in dom(CT)$
	\item there are no cycles in the subtype relation induced by CT, i.e., the
		relation <: is antisymmetric
\end{itemize}

\subsection{Typing}

The typing rules for expressions are in Table~\ref{exptyping}. An environment
$\Gamma$ is a finite mapping from variables to types, written $\bar{c}:\bar{C}$.
The typing judgment for expressions has the form $\Gamma \vdash e: C$, read ``in
the environment $\Gamma$, expression $e$ has type $C$''.

\begin{table}[h!]
	\centering
	\def\arraystretch{3}
    \caption{Expression typing}
	\begin{tabular}{cr}
		$\Gamma \vdash x:\Gamma(x)$& (T-Var)\\

		\inferrule{\Gamma \vdash e_{0}:C_{0}\qquad fields~(C_{0})=\bar{C}\
		\bar{f}}
		{\Gamma \vdash e_{0}.f_{i}:C_{i}} & (T-Field)\\

		\inferrule{\Gamma \vdash e_{0}:C_{0}\qquad
			mtypes~(m,~C_{0})=\bar{D}\rightarrow C\qquad \Gamma \vdash
		\bar{e} : \bar{C} \qquad \bar{C}~<:~\bar{D}}
		{\Gamma \vdash e_{0}.m(\bar{e}):C} & (T-Invk)\\

		\inferrule{fields(C)=\bar{D}\ \bar{f}\qquad \Gamma \vdash
		\bar{e}:\bar{C} \qquad \bar{C}~<:~\bar{D}}
		{\Gamma \vdash new\ C(\bar{e}):C} & (T-New)\\

		\inferrule{\Gamma \vdash e_{0}:D \qquad D~<:~C}
		{\Gamma \vdash (C)~e_{0}: C} & (T-UCast)\\

		\inferrule{\Gamma \vdash e_{0}:D\qquad C~<:~D \qquad C \neq D}
		{\Gamma \vdash (C)~e_{0}:C} & (T-DCast)\\

		\inferrule{\Gamma \vdash e_{0}:D\qquad C~\nless :~D \qquad D~\nless:~C 
		\qquad stupid\ warning}
		{\Gamma \vdash (C)~e_0:C} & (T-SCast)\\

	\end{tabular}
\vspace{1.5mm}
\label{exptyping}
\end{table}

\subsection{Computation}
The reduction relation is of ther form $e \rightarrow e'$, read ``expression
$e$ reduces to expression $e'$ in one step'', We write $\rightarrow *$ for the
reflexisive and transitive closure of $\rightarrow$.

The reduction rules are given in~\ref{expcomput}. There are three reduction
rules, one for field acess, one for method invocation, and one for casting.
These were already explained above. We write $[\bar{d}=\bar{x}, e=y]e_0$ for
the result of replacing $x_1$ by $d_1$, $x_2$ by $d_2, \dots, x_n$ by $d_n$, and $y$ by $e$ in
the expression $e_0$

The reduction rules may be applied at any point in an expression, so we also
need the obvious congruence rules.

\begin{table}[h!]
	\centering
	\def\arraystretch{3}
    \caption{Expression computation}
	\begin{tabular}{cr}
		\inferrule{fields~(C) = \bar{C} \bar{f}}
        {(new\ C(\bar{e})).f_i \rightarrow e_i} & (R-Field) \\

		\inferrule{mbody~(m, C) = \bar{x}.e_0}
        {(new\ C~(\bar{e})).m~(\bar{d}) \rightarrow[\bar{d}/\bar{x}, new\ C~(\bar{e})/this]e_0} & (R-Invk)\\
		\inferrule{C<:D}
        {(D)(new\ C~(\bar{e})) \rightarrow new\ C~(\bar{e})} & (R-Cast)\\
	\end{tabular}
\vspace{1.5mm}
\label{expcomput}
\end{table}

\begin{table}[h!]
	\centering
	\def\arraystretch{3}
    \caption{Congruence}
	\begin{tabular}{cr}
		\inferrule{e_0 \rightarrow e_0'}
        {e_0.f\rightarrow e_0'.f} & (RC-Field) \\
		\inferrule{e_0 \rightarrow e_0'}
        {e_0.m~(\bar{e})\rightarrow e_0'.m~(\bar{e})} & (RC-Invk-Recv) \\
		\inferrule{e_i \rightarrow e_i'}
        {e_0.m~(\dots, e_i, \dots) \rightarrow e_0'.m~(\dots, e_i, \dots)} & (RC-Invk-Arg) \\
		\inferrule{e_i \rightarrow e_i'}
        {new\ C~(\dots, e_i, \dots) \rightarrow new\ C~(\dots, e_i', \dots)} & (RC-New-Arg) \\
		\inferrule{e_0 \rightarrow e_0'}
        {(C)e_0 \rightarrow (C)e_0'} & (RC-Cast) \\

	\end{tabular}
\vspace{1.5mm}
\label{expcongr}
\end{table}

\section{Feature Featherweight Java}

This language extends FJ with constructions of \textit{feature-oriented
programming} (FOP) by new languages constructs for feature composition,
according evaluation and type rules. FFJ was proposed by Apel et.~al., in
\cite{Apel08featurefeatherweight}.

\textit{Feature-oriented programming} (FOP) aims at the modularization of
software systems in terms of features~\cite{fop}. A \textit{feature} implements
a stakeholder's requirement and is tipically an increment in program
functionality. Different variants of a software system are distinguished in
terms of their individual features.


